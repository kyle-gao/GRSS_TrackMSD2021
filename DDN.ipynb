{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOoB3U+sfnCG0TlI5Z7OXV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle-gao/GRSS_TrackMSD2021/blob/main/DDN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcdRBC3lFZvV"
      },
      "source": [
        "#https://github.com/GeoZcx/A-deeply-supervised-image-fusion-network-for-change-detection-in-remote-sensing-images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4sAKIleU-pl"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras.backend as K\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import shutil\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, \\\r\n",
        "    Conv2D, Add, Activation, Lambda\r\n",
        "from keras import backend as K\r\n",
        "from keras.activations import sigmoid\r\n",
        "\r\n",
        "from keras import applications\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, concatenate, \\\r\n",
        "    Activation, ZeroPadding2D,Conv2DTranspose,Subtract,multiply,add,UpSampling2D,PReLU\r\n",
        "from keras import  layers\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TensorBoard, CSVLogger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYunA1z8FUVb"
      },
      "source": [
        "def Conv2d_BN(x, nb_filter, kernel_size, strides=(1, 1), padding='same',with_activation = False):\r\n",
        "    x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides)(x)\r\n",
        "    x=PReLU()(x)\r\n",
        "    x=BatchNormalization(axis=3)(x)\r\n",
        "    x=Dropout(rate = 0.6)(x)\r\n",
        "    if with_activation == True:\r\n",
        "        x = Activation('relu')(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "def attach_attention_module(net, attention_module):\r\n",
        "    if attention_module == 'se_block':  # SE_block\r\n",
        "        net = se_block(net)\r\n",
        "    elif attention_module == 'cbam_block':  # CBAM_block\r\n",
        "        net = cbam_block(net)\r\n",
        "    else:\r\n",
        "        raise Exception(\"'{}' is not supported attention module!\".format(attention_module))\r\n",
        "\r\n",
        "    return net\r\n",
        "\r\n",
        "\r\n",
        "def se_block(input_feature, ratio=8):\r\n",
        "    \"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\r\n",
        "    As described in https://arxiv.org/abs/1709.01507.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\r\n",
        "    channel = input_feature.shape[channel_axis]\r\n",
        "\r\n",
        "    se_feature = GlobalAveragePooling2D()(input_feature)\r\n",
        "    se_feature = Reshape((1, 1, channel))(se_feature)\r\n",
        "    assert se_feature.shape[1:] == (1, 1, channel)\r\n",
        "    se_feature = Dense(channel // ratio,\r\n",
        "                       activation='relu',\r\n",
        "                       kernel_initializer='he_normal',\r\n",
        "                       use_bias=True,\r\n",
        "                       bias_initializer='zeros')(se_feature)\r\n",
        "    assert se_feature.shape[1:] == (1, 1, channel // ratio)\r\n",
        "    se_feature = Dense(channel,\r\n",
        "                       activation='sigmoid',\r\n",
        "                       kernel_initializer='he_normal',\r\n",
        "                       use_bias=True,\r\n",
        "                       bias_initializer='zeros')(se_feature)\r\n",
        "    assert se_feature.shape[1:] == (1, 1, channel)\r\n",
        "    if K.image_data_format() == 'channels_first':\r\n",
        "        se_feature = Permute((3, 1, 2))(se_feature)\r\n",
        "\r\n",
        "    se_feature = multiply([input_feature, se_feature])\r\n",
        "    return se_feature\r\n",
        "\r\n",
        "\r\n",
        "def cbam_block(cbam_feature, ratio=8):\r\n",
        "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\r\n",
        "    As described in https://arxiv.org/abs/1807.06521.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    cbam_feature = channel_attention(cbam_feature, ratio)\r\n",
        "    cbam_feature = spatial_attention(cbam_feature)\r\n",
        "    return cbam_feature\r\n",
        "\r\n",
        "\r\n",
        "def channel_attention(input_feature, ratio=8):\r\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\r\n",
        "    channel = input_feature.shape[channel_axis]\r\n",
        "\r\n",
        "    shared_layer_one = Dense(channel // ratio,\r\n",
        "                             activation='relu',\r\n",
        "                             kernel_initializer='he_normal',\r\n",
        "                             use_bias=True,\r\n",
        "                             bias_initializer='zeros')\r\n",
        "    shared_layer_two = Dense(channel,\r\n",
        "                             kernel_initializer='he_normal',\r\n",
        "                             use_bias=True,\r\n",
        "                             bias_initializer='zeros')\r\n",
        "\r\n",
        "    avg_pool = GlobalAveragePooling2D()(input_feature)\r\n",
        "    avg_pool = Reshape((1, 1, channel))(avg_pool)\r\n",
        "    assert avg_pool.shape[1:] == (1, 1, channel)\r\n",
        "    avg_pool = shared_layer_one(avg_pool)\r\n",
        "    assert avg_pool.shape[1:] == (1, 1, channel // ratio)\r\n",
        "    avg_pool = shared_layer_two(avg_pool)\r\n",
        "    assert avg_pool.shape[1:] == (1, 1, channel)\r\n",
        "\r\n",
        "    max_pool = GlobalMaxPooling2D()(input_feature)\r\n",
        "    max_pool = Reshape((1, 1, channel))(max_pool)\r\n",
        "    assert max_pool.shape[1:] == (1, 1, channel)\r\n",
        "    max_pool = shared_layer_one(max_pool)\r\n",
        "    assert max_pool.shape[1:] == (1, 1, channel // ratio)\r\n",
        "    max_pool = shared_layer_two(max_pool)\r\n",
        "    assert max_pool.shape[1:] == (1, 1, channel)\r\n",
        "\r\n",
        "    cbam_feature = Add()([avg_pool, max_pool])\r\n",
        "    cbam_feature = Activation('sigmoid')(cbam_feature)\r\n",
        "\r\n",
        "    if K.image_data_format() == \"channels_first\":\r\n",
        "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\r\n",
        "\r\n",
        "    return multiply([input_feature, cbam_feature])\r\n",
        "\r\n",
        "\r\n",
        "def spatial_attention(input_feature):\r\n",
        "    kernel_size = 7\r\n",
        "\r\n",
        "    if K.image_data_format() == \"channels_first\":\r\n",
        "        channel = input_feature.shape[1]\r\n",
        "        cbam_feature = Permute((2, 3, 1))(input_feature)\r\n",
        "    else:\r\n",
        "        channel = input_feature.shape[-1]\r\n",
        "        cbam_feature = input_feature\r\n",
        "\r\n",
        "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\r\n",
        "    assert avg_pool.shape[-1] == 1\r\n",
        "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\r\n",
        "    assert max_pool.shape[-1] == 1\r\n",
        "    concat = Concatenate(axis=3)([avg_pool, max_pool])\r\n",
        "    assert concat.shape[-1] == 2\r\n",
        "    cbam_feature = Conv2D(filters=1,\r\n",
        "                          kernel_size=kernel_size,\r\n",
        "                          strides=1,\r\n",
        "                          padding='same',\r\n",
        "                          activation='sigmoid',\r\n",
        "                          kernel_initializer='he_normal',\r\n",
        "                          use_bias=False)(concat)\r\n",
        "    assert cbam_feature.shape[-1] == 1\r\n",
        "\r\n",
        "    if K.image_data_format() == \"channels_first\":\r\n",
        "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\r\n",
        "\r\n",
        "    return multiply([input_feature, cbam_feature])\r\n",
        "\r\n",
        "\r\n",
        "def get_spatial_attention_map(input_feature):\r\n",
        "    kernel_size = 7\r\n",
        "    cbam_feature = input_feature\r\n",
        "\r\n",
        "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\r\n",
        "    assert avg_pool.shape[-1] == 1\r\n",
        "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\r\n",
        "    assert max_pool.shape[-1] == 1\r\n",
        "    concat = Concatenate(axis=3)([avg_pool, max_pool])\r\n",
        "    assert concat.shape[-1] == 2\r\n",
        "    cbam_feature = Conv2D(filters=1,\r\n",
        "                          kernel_size=kernel_size,\r\n",
        "                          strides=1,\r\n",
        "                          padding='same',\r\n",
        "                          activation='sigmoid',\r\n",
        "                          kernel_initializer='he_normal',\r\n",
        "                          use_bias=False)(concat)\r\n",
        "    assert cbam_feature.shape[-1] == 1\r\n",
        "\r\n",
        "    if K.image_data_format() == \"channels_first\":\r\n",
        "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\r\n",
        "\r\n",
        "    return cbam_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwG5CvxkOeMK",
        "cellView": "form"
      },
      "source": [
        "#@title DSIFN code\n",
        "\n",
        "def Conv2d_BN(x, nb_filter, kernel_size, strides=(1, 1), padding='same',with_activation = False):\n",
        "    x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides)(x)\n",
        "    x=PReLU()(x)\n",
        "    x=BatchNormalization(axis=3)(x)\n",
        "    x=Dropout(rate = 0.6)(x)\n",
        "    if with_activation == True:\n",
        "        x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def vgg16():\n",
        "    vgg_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(512,512,3))\n",
        "    model = Model(inputs=vgg_model.input, outputs = vgg_model.get_layer('block5_conv3').output)\n",
        "    model.trainable=False\n",
        "    return model\n",
        "\n",
        "def DSIFN(imsize):\n",
        "    #DFEN accepts inputs in size of imsize*imsize*3\n",
        "    vgg_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(imsize,imsize,3))\n",
        "\n",
        "    b5c3_model = Model(inputs=vgg_model.input, outputs = vgg_model.get_layer('block5_conv3').output)\n",
        "    b5c3_model.trainable=False\n",
        "\n",
        "    b4c3_model = Model(inputs=vgg_model.input, outputs = vgg_model.get_layer('block4_conv3').output)\n",
        "    b4c3_model.trainable=False\n",
        "\n",
        "    b3c3_model = Model(inputs=vgg_model.input, outputs = vgg_model.get_layer('block3_conv3').output)\n",
        "    b3c3_model.trainable=False\n",
        "\n",
        "    b2c2_model = Model(inputs=vgg_model.input, outputs = vgg_model.get_layer('block2_conv2').output)\n",
        "    b2c2_model.trainable=False\n",
        "\n",
        "    b1c2_model = Model(inputs=vgg_model.input, outputs = vgg_model.get_layer('block1_conv2').output)\n",
        "    b1c2_model.trainable=False\n",
        "\n",
        "    input_t1 = layers.Input((imsize,imsize,3), name='Input_t1')\n",
        "    input_t2 = layers.Input((imsize,imsize,3), name='Input_t2')\n",
        "\n",
        "    t1_b5c3 = b5c3_model(input_t1)\n",
        "    t2_b5c3 = b5c3_model(input_t2)\n",
        "\n",
        "    t1_b4c3 = b4c3_model(input_t1)\n",
        "    t2_b4c3 = b4c3_model(input_t2)\n",
        "\n",
        "    t1_b3c3 = b3c3_model(input_t1)\n",
        "    t2_b3c3 = b3c3_model(input_t2)\n",
        "\n",
        "    t1_b2c2 = b2c2_model(input_t1)\n",
        "    t2_b2c2 = b2c2_model(input_t2)\n",
        "\n",
        "    t1_b1c2 = b1c2_model(input_t1)\n",
        "    t2_b1c2 = b1c2_model(input_t2)\n",
        "\n",
        "    concat_b5c3 = concatenate([t1_b5c3, t2_b5c3], axis=3) #channel 1024\n",
        "    x = Conv2d_BN(concat_b5c3,imsize, 3)\n",
        "    x = Conv2d_BN(x,imsize,3)\n",
        "    attention_map_1 = get_spatial_attention_map(x)\n",
        "    x = multiply([x, attention_map_1])\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "\n",
        "    #branche1\n",
        "    branch_1 =Conv2D(1, kernel_size=1, activation='sigmoid', padding='same',name='output_32')(x)\n",
        "\n",
        "    x = Conv2DTranspose(imsize, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(x)\n",
        "    x = concatenate([x,t1_b4c3,t2_b4c3],axis=3)\n",
        "    x = channel_attention(x)\n",
        "    x = Conv2d_BN(x,imsize,3)\n",
        "    x = Conv2d_BN(x,256,3)\n",
        "    x = Conv2d_BN(x,256,3)\n",
        "    attention_map_2 = get_spatial_attention_map(x)\n",
        "    x = multiply([x, attention_map_2])\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "\n",
        "    #branche2\n",
        "    branch_2 =Conv2D(1, kernel_size=1, activation='sigmoid', padding='same',name='output_64')(x)\n",
        "\n",
        "    x = Conv2DTranspose(256, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(x)\n",
        "    x = concatenate([x,t1_b3c3,t2_b3c3],axis=3)\n",
        "    x = channel_attention(x)\n",
        "    x = Conv2d_BN(x,256,3)\n",
        "    x = Conv2d_BN(x,128,3)\n",
        "    x = Conv2d_BN(x, 128, 3)\n",
        "    attention_map_3 = get_spatial_attention_map(x)\n",
        "    x = multiply([x, attention_map_3])\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "\n",
        "    #branche3\n",
        "    branch_3 =Conv2D(1, kernel_size=1, activation='sigmoid', padding='same',name='output_128')(x)\n",
        "\n",
        "    x = Conv2DTranspose(128, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(x)\n",
        "    x = concatenate([x,t1_b2c2,t2_b2c2],axis=3)\n",
        "    x = channel_attention(x)\n",
        "    x = Conv2d_BN(x,128,3)\n",
        "    x = Conv2d_BN(x,64,3)\n",
        "    x = Conv2d_BN(x, 64, 3)\n",
        "    attention_map_4 = get_spatial_attention_map(x)\n",
        "    x = multiply([x, attention_map_4])\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "\n",
        "    #branche4\n",
        "    branch_4 =Conv2D(1, kernel_size=1, activation='sigmoid', padding='same',name='output_256')(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(x)\n",
        "    x = concatenate([x,t1_b1c2,t2_b1c2],axis=3)\n",
        "    x = channel_attention(x)\n",
        "    x = Conv2d_BN(x,64,3)\n",
        "    x = Conv2d_BN(x,32,3)\n",
        "    x = Conv2d_BN(x, 16, 3)\n",
        "    attention_map_5 = get_spatial_attention_map(x)\n",
        "    x = multiply([x, attention_map_5])\n",
        "\n",
        "    # branche5\n",
        "    branch_5 =Conv2D(1, kernel_size=1, activation='sigmoid', padding='same',name='output_imsize')(x)\n",
        "\n",
        "    DSIFN = Model(inputs=[input_t1,input_t2], outputs=[branch_1,branch_2,branch_3,branch_4,branch_5])\n",
        "\n",
        "    return DSIFN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbdAfEOxU5o4"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNSoIhZsWRpq",
        "outputId": "5c137e44-3fa0-42be-8432-95e223635a52"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAVHeP8jW6yM"
      },
      "source": [
        "shutil.unpack_archive(\"/content/drive/MyDrive/Kaggle_CoverChange.zip\",\"/content/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtVych-nS5n5"
      },
      "source": [
        "data_dir = \"/content/Kaggle_CoverChange\"\r\n",
        "list_ds = tf.data.Dataset.list_files(str(data_dir+\"/im1/*\"),shuffle=False)\r\n",
        "#dataset is made up of strings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASeQSd7UwjPO"
      },
      "source": [
        "\r\n",
        "#classes = array([ 29.,  38.,  75.,  76., 128., 150., 255.])\r\n",
        "class_dict = {(29):1, (38):2, (75):3,(76):3,128:4,150:5,(255):0} #mapping 75 and 76 to same class, i'm assuming this is an encoding issue.\r\n",
        "\r\n",
        "def to_categorical(tensor,class_dict):\r\n",
        "  #maps pixel values to categories 1,2...num_classes\r\n",
        "  for k,v in class_dict.items():\r\n",
        "    tensor[tensor==k]=v\r\n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiwDCfO1j-3g"
      },
      "source": [
        "label1_fn = \"/content/Kaggle_CoverChange/label1/00003.png\"\r\n",
        "label1 = tf.keras.preprocessing.image.load_img(label1_fn,color_mode=\"grayscale\")\r\n",
        "label1 = tf.keras.preprocessing.image.img_to_array(label1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMQDMpk9p9FM"
      },
      "source": [
        "##Testing label change conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyb7mGzSxDXK"
      },
      "source": [
        "num_classes = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sd3fkTBxlLh"
      },
      "source": [
        "##Need to convert 2 labels into 1 change label\r\n",
        "\r\n",
        "Trick: Let n be number of classes\r\n",
        "Step 1. Use an outer product between one_hot encoded labels\r\n",
        "(h,w,n),(h,w,n)->(h,w,n,n), this is gives a one hot encoded change matrix M.\r\n",
        "\r\n",
        "Eg. M = 0 except for Mij = 1 -> class i changed to class j.\r\n",
        "\r\n",
        "Step 2. Convert to categorical change -> (h,w,n,n)->(h,w,n^2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnWIcy96GeId",
        "outputId": "c884f978-ba28-4416-9395-429643f14d07"
      },
      "source": [
        "\"\"\"#converting (label1,label2)->(label changemap)\r\n",
        "#where change map has num_classes^2 (not all unique) change classes \r\n",
        "def make_label_change_dict(num_classes):\r\n",
        "  label_change_dict = {}\r\n",
        "  i = 0\r\n",
        "  for x in range(num_classes):\r\n",
        "    for y in range(num_classes):\r\n",
        "      label_change_dict[(x,y)] = i *(x!=y)\r\n",
        "      i += 1\r\n",
        "  return label_change_dict\r\n",
        "\r\n",
        "label_change_dict = make_label_change_dict(6)\r\n",
        "print(label_change_dict)\"\"\"\r\n",
        "\r\n",
        "#converting (label1,label2)->(label changemap)\r\n",
        "#where change map has num_classes^2 (not all unique) change classes \r\n",
        "def make_label_change_array(num_classes):\r\n",
        "  \"\"\"Arg:\r\n",
        "  num_classes:int, number of classes\r\n",
        "  returns:\r\n",
        "  num_classes^2 matrix of categorical change labels.\r\n",
        "  \"\"\"\r\n",
        "  label_change_arr = np.zeros((num_classes,num_classes),dtype=np.uint8)\r\n",
        "  i = 0\r\n",
        "  for x in range(num_classes):\r\n",
        "    for y in range(num_classes):\r\n",
        "      label_change_arr[x,y] = i *(x!=y)\r\n",
        "      i += 1\r\n",
        "  return label_change_arr\r\n",
        "\r\n",
        "label_change_arr = make_label_change_array(6)\r\n",
        "\r\n",
        "print(label_change_arr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  0  8  9 10 11]\n",
            " [12 13  0 15 16 17]\n",
            " [18 19 20  0 22 23]\n",
            " [24 25 26 27  0 29]\n",
            " [30 31 32 33 34  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSTX_fohoYku"
      },
      "source": [
        "def get_class_change_label(before,after,label_change_arr):\r\n",
        "  \"\"\"\r\n",
        "  Input:\r\n",
        "  two np.array or tf.tensor of shape (batch,Width,Height, num_classes) corresponding to class labeled data in one hot encoding\r\n",
        "  the arrays represent the before and after class labels for change detection\r\n",
        "  Output:\r\n",
        "  a np.array of shape (batch,Width,Height) corresponding to change labels (see figure 2)\r\n",
        "  \"\"\"\r\n",
        "  labels_combined = np.einsum(\"abd,abe->abde\",before,after)\r\n",
        "  labels_combined = labels_combined*label_change_arr\r\n",
        "  labels_combined = np.sum(labels_combined, axis=(-1,-2))\r\n",
        "  return labels_combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP6H0-H8FkJQ"
      },
      "source": [
        "label1_fn = \"/content/Kaggle_CoverChange/label1/00018.png\"\r\n",
        "label2_fn = \"/content/Kaggle_CoverChange/label2/00018.png\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaPsT5RHIlCo"
      },
      "source": [
        "##Testing change label generation trick"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ipkjfudFFiUW",
        "outputId": "d34956bc-fbdf-41d2-ce84-54e15167419b"
      },
      "source": [
        " \"\"\"   label1 = tf.keras.preprocessing.image.load_img(label1_fn, color_mode=\"grayscale\")\r\n",
        "    label1 = tf.keras.preprocessing.image.img_to_array(label1)\r\n",
        "    label1old = label1\r\n",
        "    label1 = to_categorical(label1,class_dict)\r\n",
        "    label1 = tf.expand_dims(label1,axis=0)\r\n",
        "    label1 = tf.one_hot(tf.cast(label1[:,:,:,0],dtype = tf.uint8),depth = num_classes)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    label2 = tf.keras.preprocessing.image.load_img(label2_fn, color_mode =\"grayscale\")\r\n",
        "    label2 = tf.keras.preprocessing.image.img_to_array(label2)\r\n",
        "    label1old = label1\r\n",
        "    label2 = to_categorical(label2,class_dict)\r\n",
        "    label2 = tf.expand_dims(label2,axis=0)\r\n",
        "    label2old = label2\r\n",
        "    label2 = tf.one_hot(tf.cast(label1[:,:,:,0],dtype = tf.uint8),depth=num_classes)\r\n",
        "\r\n",
        "    print(label1.shape)\r\n",
        "    print(label2.shape)\r\n",
        "    print(label1[0,300,300,:])\r\n",
        "    print(label2[0,300,300,:])\r\n",
        "    lc = np.einsum(\"abcd,abce->abcde\",label1,label2)\r\n",
        "\r\n",
        "    print(lc[0,300,300,:,:])\r\n",
        "    lc = lc * label_change_arr\r\n",
        "    print(lc[0,300,300,:,:])\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'   label1 = tf.keras.preprocessing.image.load_img(label1_fn, color_mode=\"grayscale\")\\n   label1 = tf.keras.preprocessing.image.img_to_array(label1)\\n   label1old = label1\\n   label1 = to_categorical(label1,class_dict)\\n   label1 = tf.expand_dims(label1,axis=0)\\n   label1 = tf.one_hot(tf.cast(label1[:,:,:,0],dtype = tf.uint8),depth = num_classes)\\n\\n\\n\\n\\n   label2 = tf.keras.preprocessing.image.load_img(label2_fn, color_mode =\"grayscale\")\\n   label2 = tf.keras.preprocessing.image.img_to_array(label2)\\n   label1old = label1\\n   label2 = to_categorical(label2,class_dict)\\n   label2 = tf.expand_dims(label2,axis=0)\\n   label2old = label2\\n   label2 = tf.one_hot(tf.cast(label1[:,:,:,0],dtype = tf.uint8),depth=num_classes)\\n\\n   print(label1.shape)\\n   print(label2.shape)\\n   print(label1[0,300,300,:])\\n   print(label2[0,300,300,:])\\n   lc = np.einsum(\"abcd,abce->abcde\",label1,label2)\\n\\n   print(lc[0,300,300,:,:])\\n   lc = lc * label_change_arr\\n   print(lc[0,300,300,:,:])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPl1JfAgIo3b"
      },
      "source": [
        "##Testing over"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooE716J8bpnx"
      },
      "source": [
        "def get_item(path):\r\n",
        "\r\n",
        "  #wrap function in tf.numpy_function() or tf.data.dataset.map bugs out\r\n",
        "\r\n",
        "  def _get_item(path):\r\n",
        "    \"\"\"\r\n",
        "    args:\r\n",
        "    path: Dataset.list_file dataset element\r\n",
        "\r\n",
        "    returns: \r\n",
        "    (h,w,3),(h,w,1),(h,w,3),(h,w,1) image-label 4-tuple in tf.float32 tf.Tensor\r\n",
        "    \"\"\"\r\n",
        "    fn = tf.strings.split(path,\"/\")\r\n",
        "    base_dir = tf.strings.join(fn[:-2],separator=\"/\")\r\n",
        "\r\n",
        "    image1_fn = (base_dir+\"/im1/\"+fn[-1]).numpy()\r\n",
        "    image2_fn = (base_dir+\"/im2/\"+fn[-1]).numpy()\r\n",
        "\r\n",
        "    label1_fn = (base_dir+\"/label1/\"+fn[-1]).numpy()\r\n",
        "    label2_fn = (base_dir+\"/label2/\"+fn[-1]).numpy()\r\n",
        "\r\n",
        "    #label1_fn = \"/content/Kaggle_CoverChange/label1/00018.png\"\r\n",
        "    #label2_fn = \"/content/Kaggle_CoverChange/label2/00018.png\"\r\n",
        "\r\n",
        "\r\n",
        "    image1 = tf.keras.preprocessing.image.load_img(image1_fn)\r\n",
        "    image1 = tf.keras.preprocessing.image.img_to_array(image1)\r\n",
        "\r\n",
        "    image2 = tf.keras.preprocessing.image.load_img(image2_fn)\r\n",
        "    image2 = tf.keras.preprocessing.image.img_to_array(image2)\r\n",
        "\r\n",
        "    label1 = tf.keras.preprocessing.image.load_img(label1_fn, color_mode=\"grayscale\")\r\n",
        "    label1 = tf.keras.preprocessing.image.img_to_array(label1)\r\n",
        "    label1 = to_categorical(label1,class_dict)\r\n",
        "    #label1 = tf.expand_dims(label1,axis=0)\r\n",
        "\r\n",
        "    label1 = tf.one_hot(tf.cast(label1[:,:,0],dtype = tf.uint8),depth = num_classes)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    label2 = tf.keras.preprocessing.image.load_img(label2_fn, color_mode =\"grayscale\")\r\n",
        "    label2 = tf.keras.preprocessing.image.img_to_array(label2)\r\n",
        "    label2 = to_categorical(label2,class_dict)\r\n",
        "    #label2 = tf.expand_dims(label2,axis=0)\r\n",
        "\r\n",
        "    label2 = tf.one_hot(tf.cast(label2[:,:,0],dtype = tf.uint8),depth=num_classes)\r\n",
        "\r\n",
        "    change_label = get_class_change_label(label1,label2,label_change_arr=label_change_arr)\r\n",
        "    change_label = tf.one_hot(tf.cast(change_label,dtype = tf.uint8),depth=num_classes**2-1)\r\n",
        "\r\n",
        "    #classes = array([ 29.,  38.,  75.,  76., 128., 150., 255.])\r\n",
        "    #(h,w,1) tensor of int from 1-6\r\n",
        "\r\n",
        "    return image1,image2,change_label\r\n",
        "\r\n",
        "  output = tf.numpy_function(_get_item,[path],[tf.float32,tf.float32,tf.float32])\r\n",
        "\r\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3E2boXh6fa"
      },
      "source": [
        "def preprocessing(list_ds,batch_size=8,augmentation=None):\r\n",
        "  \"\"\"\r\n",
        "  applies some preprocessing\r\n",
        "  args:\r\n",
        "  list_ds-Dataset.list_files dataset object\r\n",
        "  batch_size-int \r\n",
        "  augmentation-a function (x,y)->(x,y)\r\n",
        "  returns-batched dataset\r\n",
        "  \"\"\"\r\n",
        "  #ds = list_ds.cache() \r\n",
        "  ds = list_ds.map(get_item,num_parallel_calls=tf.data.AUTOTUNE) #(h,w,3),(h,w,1)\r\n",
        "  ds = ds.batch(batch_size)\r\n",
        "  ds = ds.shuffle(100)\r\n",
        "\r\n",
        "  if augmentation:\r\n",
        "    ds = ds.map((lambda x,y : augmentation(x,y)),num_parallel_calls=tf.data.AUTOTUNE)\r\n",
        "\r\n",
        "  \r\n",
        "  ds = ds.prefetch(8)\r\n",
        "  return ds\r\n",
        "\r\n",
        "def transform(x,y):\r\n",
        "  \"\"\"\r\n",
        "  Write your data transformations here\r\n",
        "  \"\"\"\r\n",
        "  x=tf.keras.layers.experimental.preprocessing.Rescaling(1./255.0)(x)\r\n",
        "  y=tf.one_hot(tf.cast(y[:,:,0],dtype = tf.int32),depth = 7)\r\n",
        "\r\n",
        "  return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4yuEj0k-i7T"
      },
      "source": [
        "train_ds = preprocessing(list_ds, batch_size = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHAezdgiBYbW"
      },
      "source": [
        "test2 = list(train_ds.take(1))\r\n",
        "for i in test2[0]:\r\n",
        "  print(i.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e1kj-_CnxSs"
      },
      "source": [
        "train_ds = list_ds.map(get_item,num_parallel_calls=tf.data.AUTOTUNE)\r\n",
        "test = list(train_ds.take(1)) #[(i1,i2,l1,l2)]\r\n",
        "for i in test[0]:\r\n",
        "  print(i.shape)\r\n",
        "print(np.unique(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojLlhsBJ62fB"
      },
      "source": [
        "lc = test[0][-3]\r\n",
        "l2 = test[0][-2]\r\n",
        "l1 = test[0][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw6UEHkVA7Kt"
      },
      "source": [
        "np.unique(l1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34GI1Tma_z_X"
      },
      "source": [
        "print(l1[0,300,300].numpy())\r\n",
        "print(l2[0,300,300].numpy())\r\n",
        "print(lc[0,300,300].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4imF23aAasx"
      },
      "source": [
        "np.unique(lc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBwv52Ay8iwJ"
      },
      "source": [
        "l1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ryYyH_J8zRw"
      },
      "source": [
        "plt.imshow(l1[0,:,:,0],cmap=\"YlGnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FtBoSKv8fN8"
      },
      "source": [
        "plt.imshow(l2[0,:,:,0],cmap=\"YlGnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QphqPx87aL5"
      },
      "source": [
        "plt.imshow(lc[0,:,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05q4vDOg-SwA"
      },
      "source": [
        "np.unique(lc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu4qpWMq6guQ"
      },
      "source": [
        "#load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1jzSEkZ4YZW"
      },
      "source": [
        "shutil.unpack_archive(\"/content/drive/MyDrive/vgg_512_seg_model.zip\",\"/content/Drive/Saved_Model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm7aDD4m6pIK"
      },
      "source": [
        "vgg16_512 = tf.keras.models.load_model(\"/content/Drive/Saved_Model\")\r\n",
        "vgg16_512.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrhDcrsZM3u"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPA5X1QL7z-u"
      },
      "source": [
        "feature_layers = [vgg16_512.get_layer(\"features_c1\"),vgg16_512.get_layer(\"features_c2\"),vgg16_512.get_layer('features_c3'),vgg16_512.get_layer('features_c4'),vgg16_512.get_layer('features_c5')]\r\n",
        "feature_extractor = tf.keras.models.Model(inputs = vgg16_512.input, outputs = [layer.output for layer in feature_layers])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5lT2K-xZIU4"
      },
      "source": [
        "test_tensor = tf.random.normal((1,512,512,3))\r\n",
        "test_out = feature_extractor(test_tensor)\r\n",
        "for t in test_out:\r\n",
        "  print(t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2mdKpIkXsl4"
      },
      "source": [
        "#DDN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eiduk23cFPBz"
      },
      "source": [
        "def DDN (imsize, num_classes = 6):\r\n",
        "    #if input image 512*512\r\n",
        "\r\n",
        "    vgg16_512 = tf.keras.models.load_model(\"/content/Drive/Saved_Model\")\r\n",
        "\r\n",
        "    image1 = layers.Input((imsize,imsize,3),name = \"image1\")\r\n",
        "    image2 = layers.Input((imsize,imsize,3),name = \"image2\")\r\n",
        "\r\n",
        "    feature_layers = [vgg16_512.get_layer(\"features_c1\"),vgg16_512.get_layer(\"features_c2\"),vgg16_512.get_layer('features_c3'),vgg16_512.get_layer('features_c4'),vgg16_512.get_layer('features_c5')]\r\n",
        "    feature_extractor = tf.keras.models.Model(inputs = vgg16_512.input, outputs = [layer.output for layer in feature_layers])\r\n",
        "\r\n",
        "    features_1 = feature_extractor(image1)\r\n",
        "    features_2 = feature_extractor(image1)\r\n",
        "\r\n",
        "    t1_b5c3 = features_1[4] #(None, 32, 32, 512)\r\n",
        "    t2_b5c3 = features_2[4] \r\n",
        "\r\n",
        "    t1_b4c3 = features_1[3] #(None, 64, 64, 512)\r\n",
        "    t2_b4c3 = features_2[3] \r\n",
        "    \r\n",
        "    t1_b3c3 = features_1[2] #(None, 128, 128, 256)  \r\n",
        "    t2_b3c3 = features_2[2]\r\n",
        "\r\n",
        "    t1_b2c2 = features_1[1]\r\n",
        "    t2_b2c2 = features_2[1] #(None, 256, 256, 128) \r\n",
        "\r\n",
        "    t1_b1c2 = features_1[0]\r\n",
        "    t2_b1c2 = features_2[0] #(None, 512, 512, 64)\r\n",
        "\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    pair5 = layers.Input((imsize,imsize,64*2), name='pair5') #rbg images concatenated channel-wise\r\n",
        "    pair4 = layers.Input((imsize//2,imsize//2,128*2), name='pair4')\r\n",
        "    pair3 = layers.Input((imsize//4,imsize//4,256*2), name='pair3')\r\n",
        "    pair2 = layers.Input((imsize//8,imsize//8,512*2), name='pair2')\r\n",
        "    pair1 = layers.Input((imsize//16,imsize//16,512*2), name='pair1')\r\n",
        "\r\n",
        "\r\n",
        "    t1_b5c3 = pair1[:,:,:,:3] #(None, 32, 32, 512)\r\n",
        "    t2_b5c3 = pair1[:,:,:,3:] \r\n",
        "\r\n",
        "    t1_b4c3 = pair2[:,:,:,:3] #(None, 64, 64, 512)\r\n",
        "    t2_b4c3 = pair2[:,:,:,3:] \r\n",
        "    \r\n",
        "    t1_b3c3 = pair3[:,:,:,:3] #(None, 128, 128, 256)  \r\n",
        "    t2_b3c3 = pair3[:,:,:,3:]\r\n",
        "\r\n",
        "    t1_b2c2 = pair4[:,:,:,:3]\r\n",
        "    t2_b2c2 = pair4[:,:,:,3:] #(None, 256, 256, 128) \r\n",
        "\r\n",
        "    t1_b1c2 = pair5[:,:,:,:3]\r\n",
        "    t2_b1c2 = pair5[:,:,:,3:] #(None, 512, 512, 64)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    concat_b5c3 = concatenate([t1_b5c3, t2_b5c3], axis=3) #channel 1024\r\n",
        "    x = Conv2d_BN(concat_b5c3,imsize, 3)\r\n",
        "    x = Conv2d_BN(x,imsize,3)\r\n",
        "    attention_map_1 = get_spatial_attention_map(x)\r\n",
        "    x = multiply([x, attention_map_1])\r\n",
        "    x = BatchNormalization(axis=3)(x)\r\n",
        "\r\n",
        "    #branche1\r\n",
        "    branch_1 =Conv2D(num_classes**2, kernel_size=1, activation='softmax', padding='same',name='output_32')(x)\r\n",
        "\r\n",
        "    x = Conv2DTranspose(imsize, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(x)\r\n",
        "    x = concatenate([x,t1_b4c3,t2_b4c3],axis=3)\r\n",
        "    x = channel_attention(x)\r\n",
        "    x = Conv2d_BN(x,imsize,3)\r\n",
        "    x = Conv2d_BN(x,256,3)\r\n",
        "    x = Conv2d_BN(x,256,3)\r\n",
        "    attention_map_2 = get_spatial_attention_map(x)\r\n",
        "    x = multiply([x, attention_map_2])\r\n",
        "    x = BatchNormalization(axis=3)(x)\r\n",
        "\r\n",
        "    #branche2\r\n",
        "    branch_2 =Conv2D(num_classes**2, kernel_size=1, activation='softmax', padding='same',name='output_64')(x)\r\n",
        "\r\n",
        "    x = Conv2DTranspose(256, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(x)\r\n",
        "    x = concatenate([x,t1_b3c3,t2_b3c3],axis=3)\r\n",
        "    x = channel_attention(x)\r\n",
        "    x = Conv2d_BN(x,256,3)\r\n",
        "    x = Conv2d_BN(x,128,3)\r\n",
        "    x = Conv2d_BN(x, 128, 3)\r\n",
        "    attention_map_3 = get_spatial_attention_map(x)\r\n",
        "    x = multiply([x, attention_map_3])\r\n",
        "    x = BatchNormalization(axis=3)(x)\r\n",
        "\r\n",
        "    #branche3\r\n",
        "    branch_3 =Conv2D(num_classes**2, kernel_size=1, activation='softmax', padding='same',name='output_128')(x)\r\n",
        "\r\n",
        "    x = Conv2DTranspose(128, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(x)\r\n",
        "    x = concatenate([x,t1_b2c2,t2_b2c2],axis=3)\r\n",
        "    x = channel_attention(x)\r\n",
        "    x = Conv2d_BN(x,128,3)\r\n",
        "    x = Conv2d_BN(x,128,3)\r\n",
        "    x = Conv2d_BN(x, 128, 3)\r\n",
        "    attention_map_4 = get_spatial_attention_map(x)\r\n",
        "    x = multiply([x, attention_map_4])\r\n",
        "    x = BatchNormalization(axis=3)(x)\r\n",
        "\r\n",
        "    #branche4\r\n",
        "    branch_4 =Conv2D(num_classes**2, kernel_size=1, activation='softmax', padding='same',name='output_256')(x)\r\n",
        "\r\n",
        "    x = Conv2DTranspose(64, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(x)\r\n",
        "    x = concatenate([x,t1_b1c2,t2_b1c2],axis=3)\r\n",
        "    x = channel_attention(x)\r\n",
        "    x = Conv2d_BN(x,128,3)\r\n",
        "    x = Conv2d_BN(x,64,3)\r\n",
        "    x = Conv2d_BN(x, 64, 3)\r\n",
        "    attention_map_5 = get_spatial_attention_map(x)\r\n",
        "    x = multiply([x, attention_map_5])\r\n",
        "\r\n",
        "    # branche5\r\n",
        "    branch_5 =Conv2D(num_classes**2, kernel_size=1, activation='softmax', padding='same',name='output_imsize')(x)\r\n",
        "\r\n",
        "    DDN = Model(inputs=[image1,image2], outputs=[branch_1,branch_2,branch_3,branch_4,branch_5])\r\n",
        "\r\n",
        "    return DDN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NykWcUcQzdFh"
      },
      "source": [
        "#TEST\r\n",
        "imsize = 512\r\n",
        "image1 = tf.random.normal((1,512,512,3))\r\n",
        "image2 = tf.random.normal((1,512,512,3))\r\n",
        "ddn = DDN(512,6)\r\n",
        "forward = ddn([image1,image2])\r\n",
        "for f in forward:\r\n",
        "  print (f.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-ihDr4C1Siz"
      },
      "source": [
        "ddn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzc1BlTz1e2h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}